{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming RAG Demo with LangChain, Milvus, Quix and Mistral\n",
    "\n",
    "![Streaming RAG Demo](Streaming_RAG_Demo_LangChain.png)\n",
    "\n",
    "**Everything is running on Docker with Docker Compose.**\n",
    "\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system that can:\n",
    "1. Answer questions using a vector database (Milvus)\n",
    "2. Stream new data from Kafka using Quix\n",
    "3. Update its knowledge base in real-time\n",
    "\n",
    "We'll use:\n",
    "- **LangChain**: For orchestrating the RAG pipeline\n",
    "- **Milvus**: As our vector database\n",
    "- **Ollama**: For running the LLM locally (`mistral` model)\n",
    "- **Quix**: For Kafka streaming integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import json\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize RAG Components\n",
    "\n",
    "Now we'll set up our RAG system with:\n",
    "1. Embeddings model for converting text to vectors\n",
    "2. LLM for generating responses\n",
    "3. Vector store for storing and retrieving documents\n",
    "4. RAG prompt template\n",
    "5. The complete RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rag_components():\n",
    "    \"\"\"Initialize RAG components\"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    llm = OllamaLLM(model=\"mistral\")\n",
    "    \n",
    "    vector_store = Milvus.from_texts(\n",
    "        texts=[\"Initial empty document\"],\n",
    "        embedding=embeddings,\n",
    "        connection_args={\"host\": \"localhost\", \"port\": \"19530\"},\n",
    "        collection_name=\"streaming_rag_demo\",\n",
    "        drop_old=True\n",
    "    )\n",
    "    \n",
    "    # Create RAG prompt\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return vector_store, rag_chain\n",
    "\n",
    "# Initialize our components\n",
    "vector_store, rag_chain = setup_rag_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Initial RAG System\n",
    "\n",
    "Let's test our RAG system before adding any real data. It should respond that it doesn't have relevant information since our vector store is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Query (before streaming):\n",
      "Question: What do you know about artificial intelligence developments?\n",
      "Answer:  Based on the provided context, no information regarding AI developments is available as the document is initially empty.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Query (before streaming):\")\n",
    "question = \"What do you know about artificial intelligence developments?\"\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {rag_chain.invoke(question)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Kafka Producer\n",
    "\n",
    "Now let's create a producer that will send some sample messages to Kafka. These messages will contain information that our RAG system can learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the Kakfa Topic \n",
    "\n",
    "To make sure we have a clean state, we'll delete and recreate the topic before adding some sample messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quixstreams.kafka import Producer, Consumer\n",
    "\n",
    "def cleanup_topic():\n",
    "    \"\"\"Delete and recreate the topic to ensure clean state\"\"\"\n",
    "    print(\"\\nCleaning up Kafka topic...\")\n",
    "    \n",
    "    consumer = Consumer(\n",
    "        broker_address=\"localhost:9092\",\n",
    "        consumer_group=\"rag-consumer\",\n",
    "        auto_offset_reset=\"earliest\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Try to subscribe - this will fail if topic doesn't exist\n",
    "        consumer.subscribe([\"messages\"])\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg:\n",
    "            print(\"Found existing messages, recreating topic...\")\n",
    "            consumer.close()\n",
    "            \n",
    "            # Create producer with admin rights to delete topic\n",
    "            with Producer(\n",
    "                broker_address=\"localhost:29092\",\n",
    "                extra_config={\n",
    "                    \"allow.auto.create.topics\": \"true\",\n",
    "                },\n",
    "            ) as producer:\n",
    "                producer.delete_topics([\"messages\"])\n",
    "                time.sleep(2)  # Wait for deletion\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Topic doesn't exist yet: {e}\")\n",
    "    finally:\n",
    "        consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaning up Kafka topic...\n",
      "Found existing messages, recreating topic...\n",
      "Topic doesn't exist yet: 'Producer' object has no attribute 'delete_topics'\n"
     ]
    }
   ],
   "source": [
    "cleanup_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-25 11:44:45,992] [INFO] [quixstreams] : Topics required for this application: \n",
      "[2025-02-25 11:44:46,003] [INFO] [quixstreams] : Validating Kafka topics exist and are configured correctly...\n",
      "[2025-02-25 11:44:46,024] [INFO] [quixstreams] : Kafka topics validation complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sending messages to Kafka...\n",
      "Sending: \"The latest developments in artificial intelligence have revolutionized how we approach problem solving\"\n",
      "Sending: \"Climate change poses significant challenges to global ecosystems and human societies\"\n",
      "Sending: \"Quantum computing promises to transform cryptography and drug discovery\"\n",
      "Sending: \"Sustainable energy solutions are crucial for addressing environmental concerns\"\n",
      "\n",
      "All messages sent!\n"
     ]
    }
   ],
   "source": [
    "from quixstreams import Application\n",
    "\n",
    "def get_sample_messages():\n",
    "    return [\n",
    "        {\"chat_id\": \"id1\", \"text\": \"The latest developments in artificial intelligence have revolutionized how we approach problem solving\"},\n",
    "        {\"chat_id\": \"id2\", \"text\": \"Climate change poses significant challenges to global ecosystems and human societies\"},\n",
    "        {\"chat_id\": \"id3\", \"text\": \"Quantum computing promises to transform cryptography and drug discovery\"},\n",
    "        {\"chat_id\": \"id4\", \"text\": \"Sustainable energy solutions are crucial for addressing environmental concerns\"}\n",
    "    ]\n",
    "    \n",
    "app = Application(\n",
    "    broker_address=\"localhost:9092\",\n",
    "    auto_create_topics=True\n",
    ")\n",
    "\n",
    "with app.get_producer() as producer:\n",
    "    messages = get_sample_messages()\n",
    "    print(\"\\nSending messages to Kafka...\")\n",
    "    for message in messages:\n",
    "        print(f'Sending: \"{message[\"text\"]}\"')\n",
    "        producer.produce(\n",
    "            topic=\"messages\",\n",
    "            key=message[\"chat_id\"].encode(),\n",
    "            value=json.dumps(message).encode(),\n",
    "        )\n",
    "    print(\"\\nAll messages sent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Streaming Data\n",
    "\n",
    "Now we'll consume the messages from Kafka and add them to our vector store. This simulates how our RAG system can learn from streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-25 11:45:25,663] [INFO] [quixstreams] : Starting the Application with the config: broker_address=\"{'bootstrap.servers': 'localhost:9092'}\" consumer_group=\"rag-consumer\" auto_offset_reset=\"earliest\" commit_interval=5.0s commit_every=0 processing_guarantee=\"at-least-once\"\n",
      "[2025-02-25 11:45:25,669] [INFO] [quixstreams] : Topics required for this application: \"messages\"\n",
      "[2025-02-25 11:45:25,681] [INFO] [quixstreams] : Validating Kafka topics exist and are configured correctly...\n",
      "[2025-02-25 11:45:25,732] [INFO] [quixstreams] : Kafka topics validation complete\n",
      "[2025-02-25 11:45:25,733] [INFO] [quixstreams] : Initializing state directory at \"/Users/stephen/Documents/Zilliz/talks/quix_milvus/state/rag-consumer\"\n",
      "[2025-02-25 11:45:25,736] [INFO] [quixstreams] : Waiting for incoming messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Received message: The latest developments in artificial intelligence have revolutionized how we approach problem solving\n",
      "\n",
      "Received message: Climate change poses significant challenges to global ecosystems and human societies\n",
      "\n",
      "Received message: Quantum computing promises to transform cryptography and drug discovery\n",
      "\n",
      "Received message: Sustainable energy solutions are crucial for addressing environmental concerns\n",
      "\n",
      "Received message: The latest developments in artificial intelligence have revolutionized how we approach problem solving\n",
      "\n",
      "Received message: Climate change poses significant challenges to global ecosystems and human societies\n",
      "\n",
      "Received message: Quantum computing promises to transform cryptography and drug discovery\n",
      "\n",
      "Received message: Sustainable energy solutions are crucial for addressing environmental concerns\n",
      "\n",
      "Received message: The latest developments in artificial intelligence have revolutionized how we approach problem solving\n",
      "\n",
      "Received message: Climate change poses significant challenges to global ecosystems and human societies\n",
      "\n",
      "Received message: Quantum computing promises to transform cryptography and drug discovery\n",
      "\n",
      "Received message: Sustainable energy solutions are crucial for addressing environmental concerns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-25 11:45:38,520] [INFO] [quixstreams] : Stop processing of StreamingDataFrame\n"
     ]
    }
   ],
   "source": [
    "from quixstreams import Application\n",
    "\n",
    "def process_value(row):\n",
    "    text = row[\"text\"]\n",
    "    print(f\"\\nReceived message: {text}\")\n",
    "    \n",
    "    # Add text directly to vector store\n",
    "    vector_store.add_texts([text])\n",
    "    return row\n",
    "\n",
    "app = Application(\n",
    "    broker_address=\"localhost:9092\",\n",
    "    consumer_group=\"rag-consumer\",\n",
    "    auto_offset_reset=\"earliest\"\n",
    ")\n",
    "\n",
    "input_topic = app.topic(name=\"messages\")\n",
    "\n",
    "sdf = app.dataframe(topic=input_topic)\n",
    "\n",
    "sdf = (\n",
    "    sdf.apply(process_value)\n",
    ")\n",
    "\n",
    "app.run()\n",
    "\n",
    "# Runs in a continuous loop, so interrupt the kernel after a short while"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Updated RAG System\n",
    "\n",
    "Now let's test our RAG system again. This time it should have knowledge from the streamed messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query about AI developments:\n",
      "Question: What do you know about artificial intelligence developments?\n",
      "Answer:  The latest developments in artificial intelligence (AI) have revolutionized how we approach problem solving.\n",
      "\n",
      "Query about climate change:\n",
      "Question: What information do you have about climate change?\n",
      "Answer:  The context provides information that climate change poses significant challenges to global ecosystems and human societies. There is no specific mention of the nature or extent of these challenges, but it implies that they are substantial enough to warrant concern. The text also mentions that sustainable energy solutions are crucial for addressing environmental concerns, which might be relevant in discussing potential solutions to climate change.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query about AI\n",
    "print(\"Query about AI developments:\")\n",
    "question = \"What do you know about artificial intelligence developments?\"\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {rag_chain.invoke(question)}\\n\")\n",
    "\n",
    "# Query about climate change\n",
    "print(\"Query about climate change:\")\n",
    "question = \"What information do you have about climate change?\"\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {rag_chain.invoke(question)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
