{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Source Real-Time RAG Demo with LangChain, Milvus, Ollama, Quix Streams and Apache Kafka\n",
    "\n",
    "![Streaming RAG Demo](Streaming_RAG_Demo_LangChain.png)\n",
    "\n",
    "**Everything is running on Docker with Docker Compose.**\n",
    "\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval Augmented Generation (RAG) system that can:\n",
    "1. Answer questions using a vector database ([Milvus](https://github.com/milvus-io/milvus)).\n",
    "2. Integrate streaming data containing current context using [Quix Streams](https://github.com/quixio/quix-streams).\n",
    "3. Update its knowledge base in real time.\n",
    "\n",
    "We'll use:\n",
    "- **LangChain**: For orchestrating the RAG pipeline.\n",
    "- **Milvus**: As our vector database.\n",
    "- **Ollama**: For running the LLM locally (`mistral` model).\n",
    "- **Quix Streams**: For creating the streaming data applications.\n",
    "- **Apache Kafka**: As the streaming data broker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and imports\n",
    "\n",
    "First, let's import all necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import json\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize RAG components\n",
    "\n",
    "Now we'll set up our RAG system with:\n",
    "1. Embeddings model for converting text to vectors.\n",
    "2. LLM for generating responses.\n",
    "3. Vector store for storing and retrieving documents.\n",
    "4. RAG prompt template.\n",
    "5. The complete RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tun/.pyenv/versions/3.12.9/envs/the-data-lab-webinar/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "def setup_rag_components():\n",
    "    # Initialize RAG components\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    llm = OllamaLLM(model=\"mistral\")\n",
    "    \n",
    "    vector_store = Milvus.from_texts(\n",
    "        texts=[\"Initial empty document\"],\n",
    "        embedding=embeddings,\n",
    "        connection_args={\"host\": \"localhost\", \"port\": \"19530\"},\n",
    "        collection_name=\"streaming_rag_demo\",\n",
    "        drop_old=True\n",
    "    )\n",
    "    \n",
    "    # Create RAG prompt\n",
    "    template = \"\"\"\n",
    "        Answer the question based only on the following context: {context}\n",
    "        Question: {question}\n",
    "        Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return vector_store, rag_chain\n",
    "\n",
    "# Initialize our components\n",
    "vector_store, rag_chain = setup_rag_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test initial RAG system\n",
    "\n",
    "Let's test our RAG system before adding any real data. It should respond that it doesn't have relevant information since our vector store is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Query (before integrating streaming data):\n",
      "Question: What do you know about artificial intelligence developments?\n",
      "Answer:  Based on the provided context, I don't have any specific information about the current developments in artificial intelligence. The context provided is an empty document without any relevant content regarding AI or its advancements.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Query (before integrating streaming data):\")\n",
    "question = \"What do you know about artificial intelligence developments?\"\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {rag_chain.invoke(question)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kakfa topic cleanup\n",
    "\n",
    "To make sure we have a clean state, we'll delete and recreate the topic before adding some sample messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': <Future at 0x12442d010 state=running>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from confluent_kafka.admin import AdminClient\n",
    "\n",
    "config = {\n",
    "    \"bootstrap.servers\": \"localhost:9092\",\n",
    "}\n",
    "\n",
    "admin_client = AdminClient(config)\n",
    "\n",
    "admin_client.delete_topics(topics=[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka producer setup\n",
    "\n",
    "Now let's create a producer that will send some sample messages to Kafka. These messages will contain information that our RAG system can learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-25 16:30:46,375] [INFO] [quixstreams] : Topics required for this application: \n",
      "[2025-02-25 16:30:46,376] [INFO] [quixstreams] : Validating Kafka topics exist and are configured correctly...\n",
      "[2025-02-25 16:30:46,382] [INFO] [quixstreams] : Kafka topics validation complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sending messages to Kafka...\n",
      "Sending: \"The latest developments in artificial intelligence have revolutionized how we approach problem solving\"\n",
      "Sending: \"Climate change poses significant challenges to global ecosystems and human societies\"\n",
      "Sending: \"Quantum computing promises to transform cryptography and drug discovery\"\n",
      "Sending: \"Sustainable energy solutions are crucial for addressing environmental concerns\"\n",
      "\n",
      "All messages sent!\n"
     ]
    }
   ],
   "source": [
    "from quixstreams import Application\n",
    "\n",
    "def get_sample_messages():\n",
    "    return [\n",
    "        {\"chat_id\": \"id1\", \"text\": \"The latest developments in artificial intelligence have revolutionized how we approach problem solving\"},\n",
    "        {\"chat_id\": \"id2\", \"text\": \"Climate change poses significant challenges to global ecosystems and human societies\"},\n",
    "        {\"chat_id\": \"id3\", \"text\": \"Quantum computing promises to transform cryptography and drug discovery\"},\n",
    "        {\"chat_id\": \"id4\", \"text\": \"Sustainable energy solutions are crucial for addressing environmental concerns\"}\n",
    "    ]\n",
    "    \n",
    "app = Application(\n",
    "    broker_address=\"localhost:9092\",\n",
    "    auto_create_topics=True\n",
    ")\n",
    "\n",
    "# Get producer with automatic resource cleanup\n",
    "with app.get_producer() as producer:\n",
    "    messages = get_sample_messages()\n",
    "    print(\"\\nSending messages to Kafka...\")\n",
    "    \n",
    "    for message in messages:\n",
    "        print(f'Sending: \"{message[\"text\"]}\"')\n",
    "        producer.produce(\n",
    "            topic=\"messages\",\n",
    "            key=message[\"chat_id\"].encode(),\n",
    "            value=json.dumps(message).encode(),\n",
    "        )\n",
    "        \n",
    "    print(\"\\nAll messages sent!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process streaming data\n",
    "\n",
    "Now we'll consume the messages from Kafka using Quix Streams and add them to our vector store. This simulates how our RAG system can learn from real-time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-25 16:30:49,153] [INFO] [quixstreams] : Starting the Application with the config: broker_address=\"{'bootstrap.servers': 'localhost:9092'}\" consumer_group=\"rag-consumer\" auto_offset_reset=\"earliest\" commit_interval=5.0s commit_every=0 processing_guarantee=\"at-least-once\"\n",
      "[2025-02-25 16:30:49,154] [INFO] [quixstreams] : Topics required for this application: \"messages\"\n",
      "[2025-02-25 16:30:49,162] [INFO] [quixstreams] : Validating Kafka topics exist and are configured correctly...\n",
      "[2025-02-25 16:30:49,189] [INFO] [quixstreams] : Kafka topics validation complete\n",
      "[2025-02-25 16:30:49,191] [INFO] [quixstreams] : Initializing state directory at \"/Users/tun/Dev/Git/stephen37/talks/quix_milvus/state/rag-consumer\"\n",
      "[2025-02-25 16:30:49,195] [INFO] [quixstreams] : Waiting for incoming messages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Received message: The latest developments in artificial intelligence have revolutionized how we approach problem solving\n",
      "\n",
      "Received message: Climate change poses significant challenges to global ecosystems and human societies\n",
      "\n",
      "Received message: Quantum computing promises to transform cryptography and drug discovery\n",
      "\n",
      "Received message: Sustainable energy solutions are crucial for addressing environmental concerns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-02-25 16:30:54,672] [INFO] [quixstreams] : Stop processing of StreamingDataFrame\n"
     ]
    }
   ],
   "source": [
    "from quixstreams import Application\n",
    "\n",
    "def process_value(row):\n",
    "    text = row[\"text\"]\n",
    "    print(f\"\\nReceived message: {text}\")\n",
    "    # Add text to vector store\n",
    "    vector_store.add_texts([text])\n",
    "    \n",
    "    return row\n",
    "\n",
    "app = Application(\n",
    "    broker_address=\"localhost:9092\",\n",
    "    consumer_group=\"rag-consumer\",\n",
    "    auto_offset_reset=\"earliest\"\n",
    ")\n",
    "\n",
    "input_topic = app.topic(name=\"messages\")\n",
    "\n",
    "# Create a Streaming DataFrame for every new message in the topic\n",
    "sdf = app.dataframe(topic=input_topic)\n",
    "\n",
    "sdf = sdf.apply(process_value)\n",
    "\n",
    "app.run()\n",
    "\n",
    "# NOTE: Streaming applications runs in a continuous loop.\n",
    "# You must manually interrupt the kernel after processing the \n",
    "# sample messages to ensure subsequent notebook cells can run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Updated RAG System\n",
    "\n",
    "Now let's test our RAG system again. This time it should have knowledge from the streamed messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query about AI developments:\n",
      "Question: What do you know about artificial intelligence developments?\n",
      "Answer:  The latest developments in artificial intelligence have revolutionized how we approach problem solving.\n",
      "\n",
      "Query about climate change:\n",
      "Question: What information do you have about climate change?\n",
      "Answer:  The provided context indicates that climate change poses significant challenges to global ecosystems and human societies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query about AI\n",
    "print(\"Query about AI developments:\")\n",
    "question = \"What do you know about artificial intelligence developments?\"\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {rag_chain.invoke(question)}\\n\")\n",
    "\n",
    "# Query about climate change\n",
    "print(\"Query about climate change:\")\n",
    "question = \"What information do you have about climate change?\"\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {rag_chain.invoke(question)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "the-data-lab-webinar",
   "language": "python",
   "name": "the-data-lab-webinar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
