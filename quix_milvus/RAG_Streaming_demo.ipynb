{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming RAG Demo with LangChain, Milvus, Quix and Mistral\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval-Augmented Generation (RAG) system that can:\n",
    "1. Answer questions using a vector database (Milvus)\n",
    "2. Stream new data from Kafka using Quix\n",
    "3. Update its knowledge base in real-time\n",
    "\n",
    "We'll use:\n",
    "- **LangChain**: For orchestrating the RAG pipeline\n",
    "- **Milvus**: As our vector database\n",
    "- **Ollama**: For running the LLM locally (`mistral-small` model)\n",
    "- **Quix**: For Kafka streaming integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import all necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize RAG Components\n",
    "\n",
    "Now we'll set up our RAG system with:\n",
    "1. Embeddings model for converting text to vectors\n",
    "2. LLM for generating responses\n",
    "3. Vector store for storing and retrieving documents\n",
    "4. RAG prompt template\n",
    "5. The complete RAG chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rag_components():\n",
    "    \"\"\"Initialize RAG components\"\"\"\n",
    "    # Initialize components\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    llm = OllamaLLM(model=\"mistral-small\")\n",
    "    \n",
    "    # Set up empty Milvus vector store\n",
    "    vector_store = Milvus.from_texts(\n",
    "        texts=[\"Initial empty document\"],  # Need at least one document to create collection\n",
    "        embedding=embeddings,\n",
    "        connection_args={\"host\": \"localhost\", \"port\": \"19530\"},\n",
    "        collection_name=\"streaming_rag_demo\",\n",
    "        drop_old=True\n",
    "    )\n",
    "    \n",
    "    # Create RAG prompt\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    # Create RAG chain\n",
    "    rag_chain = (\n",
    "        {\"context\": vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return vector_store, rag_chain\n",
    "\n",
    "# Initialize our components\n",
    "vector_store, rag_chain = setup_rag_components()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Initial RAG System\n",
    "\n",
    "Let's test our RAG system before adding any real data. It should respond that it doesn't have relevant information since our vector store is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial Query (before streaming):\")\n",
    "question = \"What do you know about artificial intelligence developments?\"\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {rag_chain.invoke(question)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Kafka Producer\n",
    "\n",
    "Now let's create a producer that will send some sample messages to Kafka. These messages will contain information that our RAG system can learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the Kakfa Topic \n",
    "\n",
    "To make sure we have a clean state, we'll delete and recreate the topic before adding some sample messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quixstreams.kafka import Producer, Consumer\n",
    "\n",
    "def cleanup_topic():\n",
    "    \"\"\"Delete and recreate the topic to ensure clean state\"\"\"\n",
    "    print(\"\\nCleaning up Kafka topic...\")\n",
    "    \n",
    "    consumer = Consumer(\n",
    "        broker_address=\"localhost:29092\",\n",
    "        consumer_group=\"rag-consumer\",\n",
    "        auto_offset_reset=\"earliest\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Try to subscribe - this will fail if topic doesn't exist\n",
    "        consumer.subscribe([\"messages\"])\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg:\n",
    "            print(\"Found existing messages, recreating topic...\")\n",
    "            consumer.close()\n",
    "            \n",
    "            # Create producer with admin rights to delete topic\n",
    "            with Producer(\n",
    "                broker_address=\"localhost:29092\",\n",
    "                extra_config={\n",
    "                    \"allow.auto.create.topics\": \"true\",\n",
    "                },\n",
    "            ) as producer:\n",
    "                producer.delete_topics([\"messages\"])\n",
    "                time.sleep(2)  # Wait for deletion\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Topic doesn't exist yet: {e}\")\n",
    "    finally:\n",
    "        consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quixstreams.kafka import Producer, Consumer\n",
    "\n",
    "def send_sample_messages():\n",
    "    \"\"\"Send sample messages to Kafka\"\"\"\n",
    "    messages = [\n",
    "        {\"chat_id\": \"id1\", \"text\": \"The latest developments in artificial intelligence have revolutionized how we approach problem solving\"},\n",
    "        {\"chat_id\": \"id2\", \"text\": \"Climate change poses significant challenges to global ecosystems and human societies\"},\n",
    "        {\"chat_id\": \"id3\", \"text\": \"Quantum computing promises to transform cryptography and drug discovery\"},\n",
    "        {\"chat_id\": \"id4\", \"text\": \"Sustainable energy solutions are crucial for addressing environmental concerns\"}\n",
    "    ]\n",
    "    \n",
    "    with Producer(\n",
    "        broker_address=\"localhost:29092\",\n",
    "        extra_config={\"allow.auto.create.topics\": \"true\"},\n",
    "    ) as producer:\n",
    "        print(\"\\nSending messages to Kafka...\")\n",
    "        for message in messages:\n",
    "            print(f'Sending: \"{message[\"text\"]}\"')\n",
    "            producer.produce(\n",
    "                topic=\"messages\",\n",
    "                key=message[\"chat_id\"].encode(),\n",
    "                value=json.dumps(message).encode(),\n",
    "            )\n",
    "            time.sleep(1)  # Wait for processing\n",
    "        print(\"\\nAll messages sent!\")\n",
    "\n",
    "# Send our sample messages\n",
    "send_sample_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Streaming Data\n",
    "\n",
    "Now we'll consume the messages from Kafka and add them to our vector store. This simulates how our RAG system can learn from streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stream(vector_store):\n",
    "    \"\"\"Process all messages from Kafka stream and add to RAG system\"\"\"\n",
    "    consumer = Consumer(\n",
    "        broker_address=\"localhost:29092\",\n",
    "        consumer_group=\"rag-consumer\",\n",
    "        auto_offset_reset=\"earliest\"\n",
    "    )\n",
    "    consumer.subscribe([\"messages\"])\n",
    "    \n",
    "    print(\"\\nProcessing messages from Kafka...\")\n",
    "    empty_polls = 0\n",
    "    max_empty_polls = 5  # Wait for a few empty polls before giving up\n",
    "    \n",
    "    while empty_polls < max_empty_polls:\n",
    "        msg = consumer.poll(timeout=1.0)\n",
    "        if msg is None:\n",
    "            empty_polls += 1\n",
    "            continue\n",
    "            \n",
    "        if msg.error():\n",
    "            print(f\"Consumer error: {msg.error()}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            value = json.loads(msg.value().decode())\n",
    "            text = value[\"text\"]\n",
    "            print(f\"\\nReceived message: {text}\")\n",
    "            \n",
    "            # Add text directly to vector store\n",
    "            vector_store.add_texts([text])\n",
    "            empty_polls = 0  # Reset counter when we get a message\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing message: {e}\")\n",
    "    \n",
    "    consumer.close()\n",
    "    print(\"\\nFinished processing messages from Kafka\")\n",
    "\n",
    "# Process the streaming data\n",
    "process_stream(vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Updated RAG System\n",
    "\n",
    "Now let's test our RAG system again. This time it should have knowledge from the streamed messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query about AI\n",
    "print(\"Query about AI developments:\")\n",
    "question = \"What do you know about artificial intelligence developments?\"\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {rag_chain.invoke(question)}\\n\")\n",
    "\n",
    "# Query about climate change\n",
    "print(\"Query about climate change:\")\n",
    "question = \"What information do you have about climate change?\"\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {rag_chain.invoke(question)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
