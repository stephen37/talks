{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Video RAG with Milvus and Pixtral\n",
    "\n",
    "## Goal of this Notebook\n",
    "\n",
    "In this Notebook, we will explore different techniques for video analysis and retrieval:\n",
    "\n",
    "### 1Ô∏è‚É£ **Process and Store Video Data:**\n",
    "Learn how to extract frames and audio from YouTube videos, and store this multimodal data efficiently in [Milvus](https://milvus.io/), a powerful vector database.\n",
    "\n",
    "### 2Ô∏è‚É£ **Build Multimodal Indexes:**\n",
    "Discover how to create and use indexes for both text and images using Milvus and Llama Index, enabling fast and accurate retrieval of video content.\n",
    "\n",
    "### 3Ô∏è‚É£ **Implement Retrieval Augmentation:**\n",
    "Explore techniques to fetch relevant images and context to enhance user queries, improving the quality and relevance of responses.\n",
    "\n",
    "### 4Ô∏è‚É£ **Leverage Pixtral for Multimodal Reasoning:**\n",
    "Learn to use [Pixtral](https://huggingface.co/mistralai/Pixtral-12B-2409), a state-of-the-art vision-language model from Mistral AI, for advanced multimodal reasoning and response generation.\n",
    "\n",
    "### 5Ô∏è‚É£ **Scale with Cloud-Based Inference:**\n",
    "Implement cloud-based inference using [Koyeb](https://www.koyeb.com/) to deploy Pixtral efficiently, we are using it in combination with vLLM.\n",
    "\n",
    "### üîç **Summary**\n",
    "By the end of this notebook, you'll have built a Multimodal Video RAG system that can understand and analyze video content comprehensively. You'll gain hands-on experience with Milvus for vector storage, Pixtral for multimodal reasoning, and Koyeb inference for scalable processing.\n",
    "\n",
    "This system demonstrates advanced AI capabilities including:\n",
    "- AI-powered video understanding\n",
    "- Dynamic multimodal fusion of visual, audio, and textual data\n",
    "- Context-rich prompting for improved LLM responses\n",
    "- Scalable processing of complex video content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milvus\n",
    "Milvus is an open-source vector database that powers AI applications with vector embeddings and similarity search.\n",
    "\n",
    "In this notebook, we use [Milvus Standalone](https://milvus.io/docs/install_standalone-docker.md). It is a single-machine server deployment. All components of Milvus Standalone are packed into a single Docker image, making deployment convenient. \n",
    "\n",
    "![Milvus](https://github.com/milvus-io/artwork/blob/master/horizontal/color/milvus-horizontal-color.png?raw=true)\n",
    "\n",
    "\n",
    "\n",
    "# Llama Index \n",
    "LlamaIndex is a data framework for your LLM application. It provides tools like:\n",
    "\n",
    "* Data connectors ingest your existing data from their native source and format.\n",
    "* Data indexes structure your data in intermediate representations that are easy and performant for LLMs to consume.\n",
    "* Engines provide natural language access to your data.\n",
    "* Agents are LLM-powered knowledge workers augmented by tools, from simple helper functions to API integrations and more.\n",
    "\n",
    "# Mistral AI \n",
    "Mistral AI is a research lab building LLMs and Embeddings Models, they recently released new versions of their models, Pixtral is their Multimodal Model of 12B parameters plus a 400M parameter vision encoder.\n",
    "\n",
    "### Architecture:\n",
    "Pixtral has two components: the Vision Encoder, which tokenizes images, and a Multimodal Transformer Decoder, which predicts the next text token given a sequence of text and images. The model is trained to predict the next text token on interleaved image and text data. This architecture allows Pixtral to process any number of images with arbitrary sizes in its large context window of 128K tokens.\n",
    "\n",
    "\n",
    "![Pixtral Architecture.png](https://mistral.ai/images/news/pixtral-12b/pixtral-model-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Archicture of our Multi Modal RAG system\n",
    "\n",
    "Our MultiModal RAG system processes and analyzes video content through several key stages:\n",
    "\n",
    "1. **Video Processing**:\n",
    "   - The input video is split into two streams:\n",
    "     a) Images are extracted from the video frames\n",
    "     b) Audio is converted to text using speech recognition\n",
    "\n",
    "2. **Embedding Generation**:\n",
    "   - Images are processed using CLIP to generate image embeddings\n",
    "   - The extracted text is converted into text embeddings\n",
    "\n",
    "3. **Data Storage**:\n",
    "   - Both image and text embeddings are stored in Milvus, a vector database optimized for similarity search\n",
    "\n",
    "4. **Query Processing**:\n",
    "   - User queries are converted into embeddings\n",
    "   - A semantic search is performed on both image and text data in Milvus\n",
    "\n",
    "5. **Context Retrieval**:\n",
    "   - Milvus returns similar data (images and text) based on the query\n",
    "\n",
    "6. **Response Generation**:\n",
    "   - The retrieved context is passed to Pixtral, a vision-language model (vLLM)\n",
    "   - Pixtral generates a final response based on the query and the provided context\n",
    "\n",
    "This architecture enables our system to understand and reason about video content using both visual and textual information, providing comprehensive and context-aware responses to user queries.\n",
    "\n",
    "![MultiModal RAG Architecture](resources/multimodal_rag_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qaXOp0foZWNe"
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-vector-stores-milvus\n",
    "%pip install llama-index-multi-modal-llms-mistralai llama-index-embeddings-mistralai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ziuyA5h-ZWNf"
   },
   "outputs": [],
   "source": [
    "%pip install llama-index-multi-modal-llms-openai\n",
    "%pip install llama-index-embeddings-clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oM32JRfZZWNf"
   },
   "outputs": [],
   "source": [
    "%pip install llama_index ftfy regex tqdm\n",
    "%pip install -U openai-whisper\n",
    "%pip install git+https://github.com/openai/CLIP.git\n",
    "%pip install torch torchvision\n",
    "%pip install matplotlib scikit-image\n",
    "%pip install pymilvus\n",
    "%pip install moviepy\n",
    "%pip install pytube\n",
    "%pip install pydub\n",
    "%pip install SpeechRecognition\n",
    "%pip install ffmpeg-python\n",
    "%pip install soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nw3XYAGkZWNf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import json\n",
    "from pathlib import Path\n",
    "from moviepy.editor import VideoFileClip\n",
    "import speech_recognition as sr\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import SimpleDirectoryReader, StorageContext, Settings\n",
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "from llama_index.core.schema import ImageNode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get your API Key for Mistral on https://console.mistral.ai/api-keys/\n",
    "\n",
    "Check out how to Deploy vLLM on Koyeb here: https://www.koyeb.com/deploy/vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SscG-69dZWNf"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load_dotenv reads key-value pairs from a .env file and can set them as environment variables.\n",
    "This is useful to avoid leaking your API key for example :D\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Default Embedding for Images \n",
    "\n",
    "The default embedding for images is [CLIP](https://github.com/openai/CLIP) from OpenAI. LlamaIndex is using this one by default when using Images so we do not need to define it. \n",
    "\n",
    "# Default Embeddings for Text\n",
    "\n",
    "Given that we use Mistral AI Pixtral Model, we are going to use their Embedding Models as well. This embedding model is really good at Retrieval tasks, which is very useful for RAG systems.\n",
    "\n",
    "https://docs.mistral.ai/capabilities/embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.mistralai import MistralAIEmbedding\n",
    "\n",
    "Settings.embed_model = MistralAIEmbedding(\n",
    "    \"mistral-embed\", api_key=os.getenv(\"MISTRAL_API_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ht7UqwKtZWNg"
   },
   "outputs": [],
   "source": [
    "video_path = \"https://www.youtube.com/watch?v=d_qvLDhkg00\"\n",
    "output_video_path = \"./video_data/\"\n",
    "output_folder = \"./mixed_data/\"\n",
    "output_audio_path = \"./mixed_data/output_audio.wav\"\n",
    "\n",
    "filepath = output_video_path + \"gaussian.mp4\"\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eARltNG_ZWNg"
   },
   "source": [
    "#### Download and process videos into appropriate format for generating/storing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "COoCioliZWNg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plot_images(image_paths, ax=None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(16, 9))\n",
    "        images_shown = 0\n",
    "        for img_path in image_paths:\n",
    "            if os.path.isfile(img_path):\n",
    "                image = Image.open(img_path)\n",
    "                plt.subplot(2, 3, images_shown + 1)\n",
    "                plt.imshow(image)\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                images_shown += 1\n",
    "                if images_shown >= 7:\n",
    "                    break\n",
    "        return fig\n",
    "    else:\n",
    "        if os.path.isfile(image_paths[0]):  # Assume single image when ax is provided\n",
    "            image = Image.open(image_paths[0])\n",
    "            ax.imshow(image)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Processing Functions\n",
    "\n",
    "The cell below defines several key functions for processing video content:\n",
    "\n",
    "* `download_video()`: Downloads a video from a given URL, with a fallback to direct download if YouTube-specific methods fail.\n",
    "\n",
    "* `video_to_images()`: Extracts frames from a video at 0.2 FPS and saves them as individual images.\n",
    "\n",
    "* `video_to_audio()`: Extracts the audio track from a video file.\n",
    "\n",
    "* `audio_to_text()`: Transcribes audio to text using the Whisper speech recognition model.\n",
    "\n",
    "These functions form the foundation of our video processing pipeline, enabling us to extract multimodal data (images and text) from video content for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pZY8A8_6ZWNg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict\n",
    "\n",
    "import requests\n",
    "import speech_recognition as sr\n",
    "from moviepy.editor import VideoFileClip\n",
    "from pytube import YouTube\n",
    "\n",
    "\n",
    "def download_video(url: str, output_path: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Download a video from a given url and save it to the output path.\n",
    "\n",
    "    Parameters:\n",
    "    url (str): The url of the video to download.\n",
    "    output_path (str): The path to save the video to.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the metadata of the video.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        yt = YouTube(url)\n",
    "        metadata = {\"Author\": yt.author, \"Title\": yt.title, \"Views\": str(yt.views)}\n",
    "        yt.streams.get_highest_resolution().download(\n",
    "            output_path=output_path, filename=\"input_vid.mp4\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error with pytube: {e}\")\n",
    "        print(\"Attempting direct download...\")\n",
    "\n",
    "        # Fallback: direct download\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            with open(os.path.join(output_path, \"input_vid.mp4\"), \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            metadata = {\"Author\": \"Unknown\", \"Title\": \"Unknown\", \"Views\": \"Unknown\"}\n",
    "        else:\n",
    "            raise Exception(\"Failed to download video directly.\")\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def video_to_images(video_path: str, output_folder: str) -> str:\n",
    "    local_video_path = video_path\n",
    "    if video_path.startswith(('http://', 'https://')):\n",
    "        import yt_dlp\n",
    "        ydl_opts = {\n",
    "            'format': 'best',\n",
    "            'outtmpl': os.path.join(output_folder, 'video.mp4')\n",
    "        }\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([video_path])\n",
    "        local_video_path = os.path.join(output_folder, 'video.mp4')\n",
    "    \n",
    "    clip = VideoFileClip(local_video_path)\n",
    "    clip.write_images_sequence(os.path.join(output_folder, \"frame%04d.png\"), fps=0.2)\n",
    "    return local_video_path\n",
    "\n",
    "\n",
    "def video_to_audio(video_path: str, output_audio_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Convert a video to audio and save it to the output path.\n",
    "\n",
    "    Parameters:\n",
    "    video_path (str): The path to the video file.\n",
    "    output_audio_path (str): The path to save the audio to.\n",
    "    \"\"\"\n",
    "    clip = VideoFileClip(video_path)\n",
    "    audio = clip.audio\n",
    "    audio.write_audiofile(output_audio_path)\n",
    "\n",
    "\n",
    "def audio_to_text(audio_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert audio to text using the SpeechRecognition library.\n",
    "\n",
    "    Parameters:\n",
    "    audio_path (str): The path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "    str: The text recognized from the audio.\n",
    "    \"\"\"\n",
    "    recognizer = sr.Recognizer()\n",
    "    audio = sr.AudioFile(audio_path)\n",
    "\n",
    "    with audio as source:\n",
    "        # Record the audio data\n",
    "        audio_data = recognizer.record(source)\n",
    "\n",
    "        try:\n",
    "            # Recognize the speech\n",
    "            text = recognizer.recognize_whisper(audio_data)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Speech recognition could not understand the audio.\")\n",
    "            text = \"\"\n",
    "        except sr.RequestError as e:\n",
    "            print(f\"Could not request results from service; {e}\")\n",
    "            text = \"\"\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the Video\n",
    "\n",
    "We convert our video into multiple modalities (images and text) that can be used for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T77y8QQZZWNh"
   },
   "outputs": [],
   "source": [
    "# Clean up existing files\n",
    "if os.path.exists(output_folder):\n",
    "    import shutil\n",
    "    shutil.rmtree(output_folder)\n",
    "Path(output_folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "local_video_path = video_to_images(video_path, output_folder)\n",
    "video_to_audio(local_video_path, output_audio_path)\n",
    "text_data = audio_to_text(output_audio_path)\n",
    "with open(os.path.join(output_folder, \"output_text.txt\"), \"w\") as file:\n",
    "    file.write(text_data)\n",
    "os.remove(output_audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCTG3EQYZWNh"
   },
   "source": [
    "## Instanciate Milvus and Load Data\n",
    "Milvus is a popular open-source vector database that powers AI applications with highly performant and scalable vector similarity search.\n",
    "\n",
    "* Setting the uri as a local file, e.g.`./milvus.db`, is the most convenient method, as it automatically utilizes Milvus Lite to store all data in this file.\n",
    "* If you have large scale of data, say more than a million vectors, you can set up a more performant Milvus server on [Docker or Kubernetes](https://milvus.io/docs/quickstart.md). In this setup, please use the server uri, e.g.`http://localhost:19530`, as your uri.\n",
    "* If you want to use [Zilliz Cloud](https://zilliz.com/cloud), the fully managed cloud service for Milvus, adjust the uri and token, which correspond to the [Public Endpoint and API key](https://docs.zilliz.com/docs/on-zilliz-cloud-console#cluster-details) in Zilliz Cloud.\n",
    "\n",
    "#### Create the multi-modal index\n",
    "\n",
    "`MultiModalVectorStoreIndex` supports building separate vector stores for image and text embedding vector stores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jsMnELbwZWNh"
   },
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, StorageContext\n",
    "from llama_index.core.indices import MultiModalVectorStoreIndex\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "\n",
    "text_store = MilvusVectorStore(\n",
    "    uri=\"http://127.0.0.1:19530\",\n",
    "    collection_name=\"text_collection\",\n",
    "    overwrite=True,\n",
    "    dim=1024,\n",
    ")\n",
    "image_store = MilvusVectorStore(\n",
    "    uri=\"http://127.0.0.1:19530\",\n",
    "    collection_name=\"image_collection\",\n",
    "    overwrite=True,\n",
    "    dim=512,\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=text_store, image_store=image_store\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(output_folder).load_data()\n",
    "\n",
    "index = MultiModalVectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLWQ6VQIZWNh"
   },
   "source": [
    "#### Use index as retriever to fetch top k (5 in this example) results from the multimodal vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "R_5s4_W9ZWNh"
   },
   "outputs": [],
   "source": [
    "retriever_engine = index.as_retriever(similarity_top_k=5, image_similarity_top_k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHPeC6ZYZWNh"
   },
   "source": [
    "#### Set the RAG prompt template\n",
    "\n",
    "We're preparing the prompt template for our RAG system:\n",
    "\n",
    "1. We define metadata for the video, including author, title, and view count.\n",
    "2. We create a structured prompt template (`qa_tmpl_str`) that includes:\n",
    "   - Instructions for the AI model\n",
    "   - Placeholders for context from the video\n",
    "   - Video metadata\n",
    "   - The user's query\n",
    "   \n",
    "This template will be used to format prompts for the AI model, ensuring it has all necessary information to provide accurate and contextual responses based on the video content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Aw639uNwZWNh"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "metadata_vid = {}\n",
    "# metadata_vid = {\n",
    "#     \"Author\": \"3Blue1Brown\",\n",
    "#     \"Title\": \"A pretty reason why Gaussian + Gaussian = Gaussian\",\n",
    "#     \"Views\": 803400,\n",
    "# }\n",
    "# metadata_str = json.dumps(metadata_vid)\n",
    "\n",
    "qa_tmpl_str = (\n",
    "    \"Given the provided information, including relevant images and retrieved context from the video, \\\n",
    " accurately and precisely answer the query without any additional prior knowledge.\\n\"\n",
    "    \"Please ensure honesty and responsibility.\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Context: {context_str}\\n\"\n",
    "    \"Metadata for video: {metadata_str} \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"Query: {query_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFCq4VINZWNh"
   },
   "source": [
    "#### Retrieve most similar text/image embeddings based on user query from Milvus\n",
    "\n",
    "This cell defines a `retrieve` function that:\n",
    "\n",
    "1. Takes a query string and uses the retriever engine to find relevant content\n",
    "2. Processes the retrieval results, separating them into:\n",
    "   - Image files (stored as file paths)\n",
    "   - Text content\n",
    "3. Displays text content for immediate review\n",
    "4. Returns lists of retrieved image paths and text content\n",
    "\n",
    "This function is crucial for our RAG system, as it fetches the most relevant information from our Milvus database based on the user's query, preparing both visual and textual context for the AI model to use in generating a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SArglVeWZWNh"
   },
   "outputs": [],
   "source": [
    "from llama_index.core.response.notebook_utils import display_source_node\n",
    "from llama_index.core.schema import ImageNode\n",
    "\n",
    "\n",
    "def retrieve(retriever_engine, query_str):\n",
    "    retrieval_results = retriever_engine.retrieve(query_str)\n",
    "\n",
    "    retrieved_image = []\n",
    "    retrieved_text = []\n",
    "    for res_node in retrieval_results:\n",
    "        if isinstance(res_node.node, ImageNode):\n",
    "            retrieved_image.append(res_node.node.metadata[\"file_path\"])\n",
    "        else:\n",
    "            display_source_node(res_node, source_length=200)\n",
    "            retrieved_text.append(res_node.text)\n",
    "\n",
    "    return retrieved_image, retrieved_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qC4Y2G7lZWNh"
   },
   "source": [
    "#### Fetching Multimodal Context for Our Query\n",
    "\n",
    "Here's where the magic happens! We're:\n",
    "\n",
    "1. Defining query about Gaussian functions from the video\n",
    "2. Retrieving relevant images and text snippets\n",
    "3. Bundling everything into a rich, multimodal context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SS7wRvgIZWNh",
    "outputId": "7743b999-44cb-4f37-a4ab-a235316133de"
   },
   "outputs": [],
   "source": [
    "query_str = \"Using examples from video, explain all things covered in the video regarding the gaussian function\"\n",
    "\n",
    "img, txt = retrieve(retriever_engine=retriever_engine, query_str=query_str)\n",
    "image_documents = SimpleDirectoryReader(\n",
    "    input_dir=output_folder, input_files=img\n",
    ").load_data()\n",
    "context_str = \"\".join(txt)\n",
    "plot_images(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZfCi8agZWNi",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Generate final response using Pixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GB4NbNyDZWNi",
    "outputId": "16c40e9a-7284-4e76-87bf-084b2c6363b2"
   },
   "outputs": [],
   "source": [
    "from llama_index.multi_modal_llms.mistralai import MistralAIMultiModal\n",
    "\n",
    "pixtral_llm = MistralAIMultiModal(model=\"pixtral-12b-2409\", max_new_tokens=300)\n",
    "\n",
    "response_1 = pixtral_llm.complete(\n",
    "    prompt=qa_tmpl_str.format(\n",
    "        context_str=context_str, query_str=query_str, metadata_str=metadata_str\n",
    "    ),\n",
    "    image_documents=image_documents,\n",
    ")\n",
    "\n",
    "pprint(response_1.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call Pixtral with vLLM on Koyeb\n",
    "\n",
    "This is where we use the power of Pixtral, our multimodal AI model! Here's what's happening:\n",
    "1. We set up a connection to Pixtral running on Koyeb's cloud infrastructure with vLLM.\n",
    "2. For each image, we:\n",
    "   - Encode it to base64 format\n",
    "   - Write a message combining our text query, context, and the image\n",
    "   - Send it off to Pixtral for analysis\n",
    "3. Pixtral then works its magic, interpreting both the text and image to generate a response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "def process_query_with_single_image(\n",
    "    query_str, context_str, metadata_str, image_document\n",
    "):\n",
    "    client = OpenAI(\n",
    "        base_url=os.getenv(\"KOYEB_ENDPOINT\"),\n",
    "        api_key=os.getenv(\"KOYEB_TOKEN\"),\n",
    "    )\n",
    "\n",
    "    with open(image_document.image_path, \"rb\") as image_file:\n",
    "        image_base64 = base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": qa_tmpl_str.format(\n",
    "                        context_str=context_str,\n",
    "                        query_str=query_str,\n",
    "                        metadata_str=metadata_str,\n",
    "                    ),\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"},\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"mistralai/Pixtral-12B-2409\", messages=messages, max_tokens=300\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return f\"Error processing image {image_document.image_path}: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def plot_images(image_paths, ax=None):\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(16, 9))\n",
    "        images_shown = 0\n",
    "        for img_path in image_paths:\n",
    "            if os.path.isfile(img_path):\n",
    "                image = Image.open(img_path)\n",
    "                plt.subplot(2, 3, images_shown + 1)\n",
    "                plt.imshow(image)\n",
    "                plt.xticks([])\n",
    "                plt.yticks([])\n",
    "                images_shown += 1\n",
    "                if images_shown >= 7:\n",
    "                    break\n",
    "        return fig\n",
    "    else:\n",
    "        if os.path.isfile(image_paths[0]):  # Assume single image when ax is provided\n",
    "            image = Image.open(image_paths[0])\n",
    "            ax.imshow(image)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        return ax\n",
    "\n",
    "def process_query_with_multiple_images(query_str, context_str, metadata_str, image_documents):\n",
    "    for i, img_doc in enumerate(image_documents):\n",
    "        response = process_query_with_single_image(query_str, context_str, metadata_str, img_doc)\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "        \n",
    "        plot_images([img_doc.image_path], ax=ax1)\n",
    "        ax1.set_title(f\"Frame {i+1}\")\n",
    "        \n",
    "        ax2.axis('off')\n",
    "        ax2.text(0, 1, f\"Analysis for Frame {i+1}:\\n\\n{response}\", \n",
    "                 verticalalignment='top', wrap=True, fontsize=10)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        display(fig)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        display(Markdown(\"---\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Using examples from video, explain all things covered in the video regarding the gaussian function\"\n",
    "\n",
    "metadata_str = \"{}\"\n",
    "response = process_query_with_multiple_images(\n",
    "    query_str, context_str, metadata_str, image_documents\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "In this notebook, we've explored the creation of a powerful Multimodal Video RAG system using Pixtral and Milvus. We've demonstrated how to:\n",
    "\n",
    "1. Process and store video data.\n",
    "2. Build multimodal indexes for efficient retrieval of both text and images with Milvus\n",
    "3. Implement retrieval augmentation to enhance user queries\n",
    "4. Leverage Pixtral, a state-of-the-art vision-language model, for advanced multimodal reasoning\n",
    "5. Scale our system with Koyeb and vLLM\n",
    "\n",
    "By combining those, we've created a system capable of understanding and analyzing video content."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚≠êÔ∏è Github\n",
    "We hope you found this tutorial on building a Multimodal Video RAG system with Milvus, Pixtral, and Koyeb informative and inspiring. If you enjoyed this project and found it useful, please consider giving us a star on [Github](https://github.com/milvus-io/milvus)! ‚≠ê\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ù Add me on Linkedin!\n",
    "If you have some questions related to Milvus, GenAI, etc, I am Stephen Batifol, you can add me on [LinkedIn](https://www.linkedin.com/in/stephen-batifol/) and I'll gladly help you.\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
